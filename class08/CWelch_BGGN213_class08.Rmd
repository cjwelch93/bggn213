---
title: "Machine Learning 1"
author: "Chloe J. Welch"
date: "10/22/2021"
output: pdf_document
---

# Clustering Methods

Kmeans clustering in R is done with the `kmeans()` function.
Here, we make up some data to test and learn with.

```{r}
tmp <- c(rnorm(30, 3), rnorm(30, -3))
data <- cbind(x=tmp, y=rev(tmp))
plot(data)
```

Run `kmeans()` set k(centers) to 2 (i.e. the number of clusters we want) nstart 20 (number of times we run it). Kmeans requires you to tell it how many clusters you want.

```{r}
km <- kmeans(data, centers = 2, nstart = 20)
km
```

> Q. How many points are in each cluster?

```{r}
km$cluster
```
```{r}
km$size
```
> Q. What 'component' of your result object details cluster assignment/membership?

```{r}
km$cluster
```

> Q. What 'component' of your result object details cluster center?

```{r}
km$centers
```

> Q. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points.

```{r}
plot(data, col=2)
points(km$centers, col="blue", pch=15, cex=2)
```

# hclust = Hierarchical Clustering

We will use the `hclust()` function on the same data as before and see how this method works.

```{r}
hc <- hclust(dist(data))
hc
```

hclust also contains a plot method

```{r}
plot(hc)
abline(h=7, col=2)
```

To find our membership vector, we need to "cut" the tree and for this, we use the `cutree()` function. We will tell it the height to cut at.

```{r}
cutree(hc, h=7)
```

We can also use `cutree()` and state the number of clusters we want.

```{r}
grps <- cutree(hc, k = 2)
```

```{r}
plot(data, col=grps)
```

To recap, we have learned how to use `kmeans()` which is written as kmeans(c, centers()) and `hclust()` which is written as hclust(dist(x)) and is more flexible than `kmeans()` and it also shows us something about the nature of our data.

# Principle Component Analysis (PCA)

Starting in the least complex way, we begin with looking at an x and y axis. The PC1 follows a "line of best fit" through the data points. The PC2 describes the variants that are left. These are coordinates tat describe the spread of the data.

Objectives of PCA include reducing dimensionality, to visualize multidimensional data, select the most useful variables/features, identify groupings of objects, and to identify outliers.

## PCA of UK food data - lab activity

Import data from a CSV file.

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

How many rows and columns?
```{r}
dim(x)
```
```{r}
x[, -1]
x
```

```{r}
rownames(x) <- x[, 1]
```

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names =1)
x
```

```{r}
barplot(as.matrix(x), col=rainbow(17), beside = TRUE)
```

```{r}
mycols <- rainbow(nrow(x))
pairs(x, col=mycols, pch=16)
```

## Now, we will use PCA to become more informed about our data.

Here, we will use the base R function for PCA, which is called `prcomp()`. This function wants the transpose of data.

```{r}

pca <- prcomp(t(x))
summary(pca)
```

```{r}
plot(pca)
```

The plot we want is a score plot (a.k.a. PCA plot). This is a plot of PC1 vs. PC2.

```{r}
attributes(pca)
```

We are after the pca$x component for this plot.

```{r}
plot(pca$x[,1:2])
text(pca$x[,1:2], labels=colnames(x))
```

We can also examine the PCA "loadings", which tell us how much the original variables contribute to each new PC...

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot(pca$rotation[,1], las=2)
```

## One more PCA for the day...

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

```{r}
nrow(rna.data)
```

```{r}
ncol(rna.data)
```

```{r}
colnames(rna.data)
```

```{r}
pca.rna <- prcomp(t(rna.data), scale=TRUE)
summary(pca.rna)
```

```{r}
plot(pca.rna)
```

```{r}
plot(pca.rna$x[,1:2])
text(pca.rna$x[,1:2], labels = colnames(rna.data))
```
